---
title: "CLIP Talk - Multi-Party Vocal Entrainment and the Correlates of Cooperation: An Exploration of Model Architectures"
collection: talks
type: "Talk"
permalink: /talks/2025-Spring-CLIP
venue: "University of Maryland, Language Science Center"
date: March, 2025
location: "College Park, Maryland"
---

Invited talk at the Computational Linguistics and Information Processing (CLIP) Lab, University of Maryland


**Abstract**

Human communication conveys more than linguistic content: our words, vocal characteristics, grammatical choices, and gestures entrain (or disentrain) to dynamically match those of our interlocutors as turns are exchanged, unconsciously building rapport and signalling cohesion. This entrainment has been shown to correlate with improved outcomes for tasks that require cooperation and good communication, such as psychological counselling, team activities, and group projects.

While dyadic (or two-party) entrainment has more balanced turn-taking and flow, multi-party entrainment is multi-dyadic, unbalanced and less intimate. Modelling multi-party vocal entrainment is also challenging with larger datasets and more complex acoustic feature sets. Deep neural networks are a viable solution for their ability to detect patterns in unstructured and semi-structured data without supervision, making it possible to detect the correlates of entrainment in cases where human annotations and exhaustive data-processing are not viable.

In this talk, I discuss vocal entrainment on two fronts: one, building experiments to study entrainment in non-dyadic (or multi-party) conversations among strangers engaged in cooperative tasks; and two, evaluating the efficacy of modelling two- and multi-party entrainment with unsupervised deep learning models.

Permalink:[https://languagescience.umd.edu/events/clip-talk-meghavarshini-krishnaswamy](https://languagescience.umd.edu/events/clip-talk-meghavarshini-krishnaswamy)

***

